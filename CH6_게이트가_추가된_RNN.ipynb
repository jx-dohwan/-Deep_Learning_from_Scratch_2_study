{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIWSq2ow0pgcCTwuqQSnsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/Deep_Learning_from_Scratch_2_study/blob/main/CH6_%EA%B2%8C%EC%9D%B4%ED%8A%B8%EA%B0%80_%EC%B6%94%EA%B0%80%EB%90%9C_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CH6_게이트가 추가된 RNN\n",
        "> RNN은 순환 경로를 포함하며 과거의 정보를 기역할 수 있었다. 구조가 단순하여 구현도 쉽지만 문제는 성능이 좋지 못하다. 그 원인은 시계열 데이터에서 시간적으로 멀리 떨어진 장기 의존 관계를 잘 학습할 수 없다는데 있다.<br><br> 요즘에는 RNN대신에 LSTM이나 GRU라는 계층이 주로 쓰인다. LSTM이나 GRU에는 게이트라는 구조가 더해져 있는데 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있다.<br><br> 이번장에서는 RNN의 문제점을 알아보고, 이를 대신하는 계층인 LSTM과 GRU와 같은 게이트가 추가된 RNN을 소개한다. 특히 장기기억을 가능하게 하는 메커니즘을 이해해본다."
      ],
      "metadata": {
        "id": "g79O2LcQMDHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e77V8oMXL9eN"
      },
      "outputs": [],
      "source": []
    }
  ]
}